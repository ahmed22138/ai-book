"use strict";(self.webpackChunkai_textbook_frontend=self.webpackChunkai_textbook_frontend||[]).push([[791],{1937:function(e,n,t){t.r(n),t.d(n,{assets:function(){return a},contentTitle:function(){return l},default:function(){return p},frontMatter:function(){return o},metadata:function(){return i},toc:function(){return c}});var i=JSON.parse('{"id":"perception/week-6-slam","title":"Week 6: SLAM & Localization","description":"The Navigation Challenge","source":"@site/docs/02-perception/week-6-slam.mdx","sourceDirName":"02-perception","slug":"/perception/week-6-slam","permalink":"/ai-textbook/docs/perception/week-6-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-textbook/tree/main/docs/02-perception/week-6-slam.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Week 5: 3D Perception & Point Clouds","permalink":"/ai-textbook/docs/perception/week-5-3d-perception"},"next":{"title":"Week 7: Path Planning","permalink":"/ai-textbook/docs/control/week-7-path-planning"}}'),s=t(4848),r=t(8453);const o={sidebar_position:3},l="Week 6: SLAM & Localization",a={},c=[{value:"The Navigation Challenge",id:"the-navigation-challenge",level:2},{value:"Why SLAM Matters",id:"why-slam-matters",level:3},{value:"Part 1: Localization Problem",id:"part-1-localization-problem",level:2},{value:"Dead Reckoning (Odometry)",id:"dead-reckoning-odometry",level:3},{value:"Probabilistic Localization",id:"probabilistic-localization",level:3},{value:"Part 2: Mapping Problem",id:"part-2-mapping-problem",level:2},{value:"Occupancy Grid Maps",id:"occupancy-grid-maps",level:3},{value:"Part 3: Complete SLAM System",id:"part-3-complete-slam-system",level:2},{value:"Real-World: Visual SLAM (Feature-Based)",id:"real-world-visual-slam-feature-based",level:2},{value:"SLAM Challenges &amp; Solutions",id:"slam-challenges--solutions",level:2},{value:"Week 6 Learning Outcomes",id:"week-6-learning-outcomes",level:2},{value:"Key Terminology",id:"key-terminology",level:2},{value:"Discussion Questions",id:"discussion-questions",level:2},{value:"Hands-On Activity",id:"hands-on-activity",level:2},{value:"Resources for Deeper Learning",id:"resources-for-deeper-learning",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-6-slam--localization",children:"Week 6: SLAM & Localization"})}),"\n",(0,s.jsx)(n.h2,{id:"the-navigation-challenge",children:"The Navigation Challenge"}),"\n",(0,s.jsx)(n.p,{children:"Imagine deploying a robot in a warehouse it has never seen before. Two fundamental questions arise:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Where am I?"})," (Localization)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"What does the world around me look like?"})," (Mapping)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This week, we tackle ",(0,s.jsx)(n.strong,{children:"SLAM"})," (Simultaneous Localization and Mapping)\u2014the foundation of autonomous robot navigation."]}),"\n",(0,s.jsx)(n.h3,{id:"why-slam-matters",children:"Why SLAM Matters"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Unknown environments"}),": Robots must navigate without pre-built maps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error accumulation"}),": Dead reckoning (wheel encoders) drifts over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time requirements"}),": Navigation decisions happen continuously"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource constraints"}),": Mobile robots have limited computation"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"part-1-localization-problem",children:"Part 1: Localization Problem"}),"\n",(0,s.jsx)(n.h3,{id:"dead-reckoning-odometry",children:"Dead Reckoning (Odometry)"}),"\n",(0,s.jsx)(n.p,{children:"The simplest approach: track robot position using wheel encoders."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class RobotOdometry:\n    def __init__(self, wheel_radius=0.05, wheel_base=0.3):\n        """Initialize odometry"""\n        self.wheel_radius = wheel_radius\n        self.wheel_base = wheel_base\n\n        self.x = 0\n        self.y = 0\n        self.theta = 0  # Heading angle\n\n        self.last_left_ticks = 0\n        self.last_right_ticks = 0\n\n    def update(self, left_encoder_ticks, right_encoder_ticks, dt=0.01):\n        """Update robot pose from encoder ticks"""\n\n        # Compute distance traveled by each wheel\n        left_distance = (left_encoder_ticks - self.last_left_ticks) * \\\n                       self.wheel_radius\n        right_distance = (right_encoder_ticks - self.last_right_ticks) * \\\n                        self.wheel_radius\n\n        self.last_left_ticks = left_encoder_ticks\n        self.last_right_ticks = right_encoder_ticks\n\n        # Compute forward and rotational motion\n        forward = (left_distance + right_distance) / 2\n        rotation = (right_distance - left_distance) / self.wheel_base\n\n        # Update heading\n        self.theta += rotation\n\n        # Update position\n        self.x += forward * np.cos(self.theta)\n        self.y += forward * np.sin(self.theta)\n\n        return self.x, self.y, self.theta\n\n    def get_pose(self):\n        """Return current pose estimate"""\n        return np.array([self.x, self.y, self.theta])\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Wheel slip, uneven terrain, and systematic errors cause ",(0,s.jsx)(n.strong,{children:"drift"})," over time."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Ideal path:     \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502     \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n\nActual (drift):  \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502     \u2514\u2500\u2500  \u2190 Accumulating error\n"})}),"\n",(0,s.jsx)(n.h3,{id:"probabilistic-localization",children:"Probabilistic Localization"}),"\n",(0,s.jsxs)(n.p,{children:["Instead of trusting a single estimate, maintain a ",(0,s.jsx)(n.strong,{children:"probability distribution"})," over possible robot positions."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ParticleFilter:\n    def __init__(self, n_particles=1000, world_size=(10, 10)):\n        """Initialize particle filter for 2D localization"""\n\n        self.n_particles = n_particles\n        self.world_size = world_size\n\n        # Create particles at random initial positions\n        self.particles = np.random.uniform(\n            [0, 0, -np.pi],\n            [world_size[0], world_size[1], np.pi],\n            (n_particles, 3)\n        )\n\n        # Initialize weights uniformly\n        self.weights = np.ones(n_particles) / n_particles\n\n    def predict(self, control_linear, control_angular, noise_level=0.1):\n        """Predict particles given control commands"""\n\n        for i in range(self.n_particles):\n            # Add control\n            x, y, theta = self.particles[i]\n\n            x += control_linear * np.cos(theta)\n            y += control_linear * np.sin(theta)\n            theta += control_angular\n\n            # Add process noise (Gaussian)\n            x += np.random.normal(0, noise_level)\n            y += np.random.normal(0, noise_level)\n            theta += np.random.normal(0, noise_level * 0.1)\n\n            self.particles[i] = [x, y, theta]\n\n    def update(self, sensor_reading, measurement_model):\n        """Update particle weights based on sensor observation"""\n\n        for i in range(self.n_particles):\n            # Compute likelihood of observation given particle pose\n            self.weights[i] *= measurement_model(\n                self.particles[i], sensor_reading\n            )\n\n        # Normalize weights\n        self.weights /= np.sum(self.weights)\n\n    def resample(self):\n        """Resample particles (discard low-weight, duplicate high-weight)"""\n\n        indices = np.random.choice(\n            self.n_particles,\n            self.n_particles,\n            p=self.weights,\n            replace=True\n        )\n\n        self.particles = self.particles[indices]\n        self.weights = np.ones(self.n_particles) / self.n_particles\n\n    def estimate_pose(self):\n        """Compute expected pose from particle distribution"""\n\n        # Weighted average\n        mean_pose = np.average(self.particles, axis=0, weights=self.weights)\n        return mean_pose\n'})}),"\n",(0,s.jsx)(n.h2,{id:"part-2-mapping-problem",children:"Part 2: Mapping Problem"}),"\n",(0,s.jsx)(n.h3,{id:"occupancy-grid-maps",children:"Occupancy Grid Maps"}),"\n",(0,s.jsxs)(n.p,{children:["Divide the world into a 2D grid. Each cell is either ",(0,s.jsx)(n.strong,{children:"occupied"})," (obstacle) or ",(0,s.jsx)(n.strong,{children:"free"})," (traversable)."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class OccupancyGrid:\n    def __init__(self, width=100, height=100, resolution=0.1):\n        """Initialize occupancy grid map"""\n\n        self.width = width          # meters\n        self.height = height        # meters\n        self.resolution = resolution  # meters/cell\n\n        self.cells_x = int(width / resolution)\n        self.cells_y = int(height / resolution)\n\n        # Store occupancy as log-odds\n        # log(p / (1-p)) where p = probability occupied\n        self.grid = np.zeros((self.cells_y, self.cells_x))\n\n    def world_to_grid(self, x, y):\n        """Convert world coordinates to grid cell"""\n        cell_x = int(x / self.resolution)\n        cell_y = int(y / self.resolution)\n\n        # Clamp to valid range\n        cell_x = max(0, min(self.cells_x - 1, cell_x))\n        cell_y = max(0, min(self.cells_y - 1, cell_y))\n\n        return cell_x, cell_y\n\n    def update_cell(self, x, y, occupied, confidence=1.0):\n        """Update cell occupancy"""\n\n        cell_x, cell_y = self.world_to_grid(x, y)\n\n        # Update log-odds\n        if occupied:\n            self.grid[cell_y, cell_x] += confidence\n        else:\n            self.grid[cell_y, cell_x] -= confidence\n\n        # Clamp log-odds to reasonable range\n        self.grid[cell_y, cell_x] = np.clip(self.grid[cell_y, cell_x], -5, 5)\n\n    def update_from_lidar(self, robot_pose, lidar_points, max_range=10):\n        """Update grid from LIDAR scan"""\n\n        robot_x, robot_y, robot_theta = robot_pose\n\n        # Mark all LIDAR hits as occupied\n        for point in lidar_points:\n            # Transform LIDAR point to world frame\n            point_x = point[0]\n            point_y = point[1]\n\n            world_x = robot_x + point_x * np.cos(robot_theta) - \\\n                     point_y * np.sin(robot_theta)\n            world_y = robot_y + point_x * np.sin(robot_theta) + \\\n                     point_y * np.cos(robot_theta)\n\n            self.update_cell(world_x, world_y, occupied=True, confidence=0.9)\n\n        # Raycasting: mark cells between robot and hits as free\n        for point in lidar_points:\n            point_x = point[0]\n            point_y = point[1]\n\n            world_x = robot_x + point_x * np.cos(robot_theta) - \\\n                     point_y * np.sin(robot_theta)\n            world_y = robot_y + point_x * np.sin(robot_theta) + \\\n                     point_y * np.cos(robot_theta)\n\n            # Raycasting: walk from robot to point\n            distance = np.sqrt(point_x**2 + point_y**2)\n            for d in np.arange(0, distance, self.resolution):\n                ray_x = robot_x + d * np.cos(robot_theta)\n                ray_y = robot_y + d * np.sin(robot_theta)\n\n                self.update_cell(ray_x, ray_y, occupied=False, confidence=0.1)\n\n    def is_occupied(self, x, y):\n        """Query if cell is occupied"""\n        cell_x, cell_y = self.world_to_grid(x, y)\n        # Positive log-odds \u2192 likely occupied\n        return self.grid[cell_y, cell_x] > 0\n\n    def visualize(self):\n        """Visualize map as image"""\n        # Convert log-odds to probabilities\n        prob = 1 / (1 + np.exp(-self.grid))\n        return (prob * 255).astype(np.uint8)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"part-3-complete-slam-system",children:"Part 3: Complete SLAM System"}),"\n",(0,s.jsx)(n.p,{children:"Now we combine localization + mapping:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SimpleSLAM:\n    def __init__(self):\n        self.particle_filter = ParticleFilter(n_particles=500)\n        self.occupancy_grid = OccupancyGrid(width=50, height=50)\n        self.odometry = RobotOdometry()\n\n    def step(self, left_encoder_ticks, right_encoder_ticks, lidar_scan):\n        """Single SLAM update step"""\n\n        dt = 0.1  # 10 Hz\n\n        # 1. PREDICT: Update particle positions using odometry\n        # (Odometry gives control commands from encoder differences)\n        forward, rotation = self.odometry.estimate_motion(\n            left_encoder_ticks, right_encoder_ticks\n        )\n\n        self.particle_filter.predict(\n            control_linear=forward,\n            control_angular=rotation\n        )\n\n        # 2. UPDATE: Correct particles using LIDAR observation\n        def measurement_model(particle_pose, lidar_scan):\n            """Compute likelihood of LIDAR scan given pose"""\n\n            # For simplicity: scan matching\n            # (In real SLAM: use feature-based or ICP matching)\n\n            predicted_scan = self.occupancy_grid.predict_lidar(\n                particle_pose\n            )\n\n            # Compare to actual scan\n            distance_error = np.sum((predicted_scan - lidar_scan)**2)\n\n            # Convert to probability (smaller error = higher probability)\n            likelihood = np.exp(-distance_error / 100)\n\n            return likelihood\n\n        self.particle_filter.update(lidar_scan, measurement_model)\n\n        # 3. RESAMPLE: Eliminate low-probability particles\n        self.particle_filter.resample()\n\n        # 4. MAP UPDATE: Update occupancy grid with best estimate\n        best_pose = self.particle_filter.estimate_pose()\n        self.occupancy_grid.update_from_lidar(best_pose, lidar_scan)\n\n        return best_pose\n\n    def get_map(self):\n        """Return current map estimate"""\n        return self.occupancy_grid.visualize()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"real-world-visual-slam-feature-based",children:"Real-World: Visual SLAM (Feature-Based)"}),"\n",(0,s.jsxs)(n.p,{children:["Modern SLAM often uses ",(0,s.jsx)(n.strong,{children:"visual features"})," instead of LIDAR:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VisualSLAM:\n    def __init__(self):\n        self.feature_extractor = SIFT()  # Or ORB, AKAZE\n        self.pose_graph = {}  # Poses and observations\n        self.landmark_map = {}  # 3D landmark positions\n        self.current_pose = np.eye(4)  # 4\xd74 transformation matrix\n\n    def process_image(self, image, depth_image):\n        """Process image for visual SLAM"""\n\n        # 1. Extract features\n        keypoints, descriptors = self.feature_extractor.detect_and_compute(image)\n\n        # 2. Match to previous frame\n        if hasattr(self, \'last_descriptors\'):\n            matches = self.match_features(descriptors, self.last_descriptors)\n\n            # 3. Estimate motion (PnP: Perspective-n-Point)\n            motion = self.estimate_motion_from_matches(\n                matches, depth_image\n            )\n\n            # 4. Update pose\n            self.current_pose = motion @ self.current_pose\n\n        # 5. Create or update landmarks\n        self.update_landmarks(keypoints, depth_image)\n\n        # Store for next frame\n        self.last_descriptors = descriptors\n\n    def estimate_motion_from_matches(self, matches, depth_image):\n        """Estimate camera motion from feature matches"""\n\n        # Use matched features and depth to get 3D positions\n        # Solve PnP problem to find camera transformation\n\n        from cv2 import solvePnP\n\n        object_points = []\n        image_points = []\n\n        for match in matches:\n            kp1, kp2 = match\n            # Get 3D position from depth\n            x, y = int(kp1.pt[0]), int(kp1.pt[1])\n            z = depth_image[y, x]\n\n            if z > 0:\n                object_points.append([x * z / 500, y * z / 500, z])\n                image_points.append([kp2.pt[0], kp2.pt[1]])\n\n        if len(object_points) > 4:\n            success, rvec, tvec = solvePnP(\n                np.array(object_points),\n                np.array(image_points),\n                camera_matrix=self.camera_intrinsics,\n                distCoeffs=None\n            )\n\n            if success:\n                # Convert rotation vector to matrix\n                rotation_matrix, _ = cv2.Rodrigues(rvec)\n\n                # Create 4\xd74 transformation matrix\n                transformation = np.eye(4)\n                transformation[:3, :3] = rotation_matrix\n                transformation[:3, 3] = tvec\n\n                return transformation\n\n        return np.eye(4)  # No motion\n'})}),"\n",(0,s.jsx)(n.h2,{id:"slam-challenges--solutions",children:"SLAM Challenges & Solutions"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Challenge"}),(0,s.jsx)(n.th,{children:"Solution"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Loop closure"})}),(0,s.jsx)(n.td,{children:"Detect when robot revisits location, correct accumulated drift"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Dynamic environments"})}),(0,s.jsx)(n.td,{children:"Filter out moving objects, track only static features"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Low texture areas"})}),(0,s.jsx)(n.td,{children:"Use LIDAR + cameras for redundancy"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Computational cost"})}),(0,s.jsx)(n.td,{children:"Keyframe-based SLAM (process sparse subset of images)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Initialization"})}),(0,s.jsx)(n.td,{children:"Stereo baseline or IMU for initial depth/motion"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"week-6-learning-outcomes",children:"Week 6 Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this week, you should be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Explain"})," why localization and mapping are coupled problems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," dead reckoning and understand its limitations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design"})," particle filter for probabilistic localization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build"})," occupancy grid maps from sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate"})," localization + mapping into complete SLAM system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apply"})," SLAM to autonomous navigation"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Term"}),(0,s.jsx)(n.th,{children:"Definition"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"SLAM"})}),(0,s.jsx)(n.td,{children:"Simultaneous Localization And Mapping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Odometry"})}),(0,s.jsx)(n.td,{children:"Robot pose estimate from wheel encoders (drifts over time)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Particle filter"})}),(0,s.jsx)(n.td,{children:"Probabilistic localization maintaining distribution over poses"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Occupancy grid"})}),(0,s.jsx)(n.td,{children:"2D map grid where cells are occupied or free"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Loop closure"})}),(0,s.jsx)(n.td,{children:"Detecting revisited locations to correct drift"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Pose graph"})}),(0,s.jsx)(n.td,{children:"Network of robot poses connected by constraints"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Landmark"})}),(0,s.jsx)(n.td,{children:"Distinctive feature used for localization and mapping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Scan matching"})}),(0,s.jsx)(n.td,{children:"Aligning sensor scans to estimate motion (ICP variant)"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"discussion-questions",children:"Discussion Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Comparison"}),": When would you use particle filter SLAM vs. visual SLAM? What are tradeoffs?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Error growth"}),": Dead reckoning error grows with distance (~1% per meter). What happens after 100m of travel?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Loop closure"}),": Robot explores a square room and returns to start. How do you detect this? How do you use it to correct drift?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Computational limits"}),": A robot processes 10 Hz LIDAR scans. Each scan takes 5ms to process. Is SLAM feasible?"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-activity",children:"Hands-On Activity"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implement simple particle filter localization"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Download a dataset with robot odometry + landmarks (KITTI, Freiburg dataset)"}),"\n",(0,s.jsxs)(n.li,{children:["Implement particle filter:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Initialize particles randomly"}),"\n",(0,s.jsx)(n.li,{children:"Predict step (add odometry + noise)"}),"\n",(0,s.jsx)(n.li,{children:"Update step (compute likelihood based on landmarks)"}),"\n",(0,s.jsx)(n.li,{children:"Resample particles"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Plot particle cloud over true trajectory"}),"\n",(0,s.jsx)(n.li,{children:"Measure accuracy (RMS error in meters)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"resources-for-deeper-learning",children:"Resources for Deeper Learning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Book"}),': "Probabilistic Robotics" - Thrun, Burgard, Fox (definitive SLAM reference)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Framework"}),": ROS with package: ",(0,s.jsx)(n.code,{children:"slam_toolbox"}),", ",(0,s.jsx)(n.code,{children:"cartographer"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Course"}),': "Robot Mapping" - University of Freiburg (freely available)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dataset"}),": Freiburg SLAM dataset with ground truth"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Paper"}),': "ORB-SLAM: a Versatile and Accurate Monocular SLAM System" - Mur-Artal et al.']}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": Module 3 - Motion Planning & Navigation"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,s.jsx)(n.strong,{children:"Tip"}),": Visualize your particles! In low-quality visualizations, particles spread out uniformly. With good sensor data, they converge to the true pose."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:function(e,n,t){t.d(n,{R:function(){return o},x:function(){return l}});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);