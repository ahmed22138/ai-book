"use strict";(self.webpackChunkai_textbook_frontend=self.webpackChunkai_textbook_frontend||[]).push([[614],{7963:function(e,n,i){i.r(n),i.d(n,{assets:function(){return a},contentTitle:function(){return l},default:function(){return p},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return c}});var s=JSON.parse('{"id":"perception/week-5-3d-perception","title":"Week 5: 3D Perception & Point Clouds","description":"From 2D Images to 3D Understanding","source":"@site/docs/02-perception/week-5-3d-perception.mdx","sourceDirName":"02-perception","slug":"/perception/week-5-3d-perception","permalink":"/ai-textbook/docs/perception/week-5-3d-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-textbook/tree/main/docs/02-perception/week-5-3d-perception.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Week 4: Computer Vision Fundamentals","permalink":"/ai-textbook/docs/perception/week-4-computer-vision"},"next":{"title":"Week 6: SLAM & Localization","permalink":"/ai-textbook/docs/perception/week-6-slam"}}'),t=i(4848),r=i(8453);const o={sidebar_position:2},l="Week 5: 3D Perception & Point Clouds",a={},c=[{value:"From 2D Images to 3D Understanding",id:"from-2d-images-to-3d-understanding",level:2},{value:"Why 3D Matters",id:"why-3d-matters",level:3},{value:"Part 1: Depth Sensing Technologies",id:"part-1-depth-sensing-technologies",level:2},{value:"Depth Camera Types",id:"depth-camera-types",level:3},{value:"Part 2: Point Clouds",id:"part-2-point-clouds",level:2},{value:"Point Cloud Representation",id:"point-cloud-representation",level:3},{value:"Point Cloud Processing Pipeline",id:"point-cloud-processing-pipeline",level:3},{value:"Part 3: 3D Object Recognition",id:"part-3-3d-object-recognition",level:2},{value:"Iterative Closest Point (ICP) Alignment",id:"iterative-closest-point-icp-alignment",level:3},{value:"Real-World Example: Bin Picking",id:"real-world-example-bin-picking",level:2},{value:"Week 5 Learning Outcomes",id:"week-5-learning-outcomes",level:2},{value:"Key Terminology",id:"key-terminology",level:2},{value:"Discussion Questions",id:"discussion-questions",level:2},{value:"Hands-On Activity",id:"hands-on-activity",level:2},{value:"Resources for Deeper Learning",id:"resources-for-deeper-learning",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-5-3d-perception--point-clouds",children:"Week 5: 3D Perception & Point Clouds"})}),"\n",(0,t.jsx)(n.h2,{id:"from-2d-images-to-3d-understanding",children:"From 2D Images to 3D Understanding"}),"\n",(0,t.jsxs)(n.p,{children:["Last week we explored 2D vision\u2014detecting objects in flat images. This week, we extend to ",(0,t.jsx)(n.strong,{children:"3D perception"}),", enabling robots to understand spatial relationships, depth, and complete 3D structures."]}),"\n",(0,t.jsx)(n.h3,{id:"why-3d-matters",children:"Why 3D Matters"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation"}),": A robot arm needs 3D coordinates to reach objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation"}),": Understanding 3D obstacles prevents collisions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasping"}),": Knowing object shape in 3D improves grasp planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scene understanding"}),": Full 3D models enable reasoning about space"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"part-1-depth-sensing-technologies",children:"Part 1: Depth Sensing Technologies"}),"\n",(0,t.jsx)(n.h3,{id:"depth-camera-types",children:"Depth Camera Types"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Stereo Vision"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Two cameras viewing same scene from different angles"}),"\n",(0,t.jsx)(n.li,{children:"Like human eyes \u2192 enables depth perception"}),"\n",(0,t.jsx)(n.li,{children:"Compute disparity (difference in pixel positions)"}),"\n",(0,t.jsx)(n.li,{children:"Pros: Passive, high resolution"}),"\n",(0,t.jsx)(n.li,{children:"Cons: Fails in texture-less areas, computationally intensive"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Time-of-Flight (ToF)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Emit light pulse, measure return time"}),"\n",(0,t.jsx)(n.li,{children:"Distance = (speed of light \xd7 time) / 2"}),"\n",(0,t.jsx)(n.li,{children:"Pros: Works on any surface, real-time"}),"\n",(0,t.jsx)(n.li,{children:"Cons: Limited range, affected by ambient light"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Structured Light"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Project known pattern (checkerboard, dots)"}),"\n",(0,t.jsx)(n.li,{children:"Measure pattern deformation to compute depth"}),"\n",(0,t.jsx)(n.li,{children:"Pros: Works on texture-less surfaces"}),"\n",(0,t.jsx)(n.li,{children:"Cons: Fails in sunlight (infrared interferes)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"LIDAR"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Laser-based scanning (covered in Week 2)"}),"\n",(0,t.jsx)(n.li,{children:"Returns millions of 3D points"}),"\n",(0,t.jsx)(n.li,{children:"Pros: Long range, accurate"}),"\n",(0,t.jsx)(n.li,{children:"Cons: Expensive, overkill for close manipulation"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DepthCamera:\n    def __init__(self, camera_type=\'rgb-d\'):\n        """Initialize depth camera"""\n        self.camera_type = camera_type\n\n        if camera_type == \'stereo\':\n            self.baseline = 0.065  # 65mm between stereo cameras\n            self.focal_length = 525  # Pixels\n\n        elif camera_type == \'tof\':\n            self.speed_of_light = 3e8  # m/s\n            self.modulation_freq = 20e6  # Hz\n\n    def stereo_depth(self, left_image, right_image):\n        """Compute depth from stereo pair"""\n\n        # 1. Rectify images (align to same rows)\n        left_rect, right_rect = self.rectify_stereo(left_image, right_image)\n\n        # 2. Compute disparity map\n        disparity_map = self.compute_disparity(left_rect, right_rect)\n\n        # 3. Convert disparity to depth\n        # depth = (baseline \xd7 focal_length) / disparity\n        depth = (self.baseline * self.focal_length) / (disparity_map + 1e-6)\n\n        return depth\n\n    def compute_disparity(self, left, right):\n        """Find matching pixels between stereo images"""\n        # For each pixel in left, find best match in right\n        # Simplified: actual implementations use more sophisticated matching\n\n        disparity = np.zeros((left.shape[0], left.shape[1]))\n\n        for y in range(left.shape[0]):\n            for x in range(left.shape[1]):\n                left_patch = left[y:y+9, x:x+9]  # 9\xd79 patch\n\n                best_distance = float(\'inf\')\n                best_disparity = 0\n\n                # Search window (typically \xb160 pixels)\n                for dx in range(-60, 61):\n                    x_right = x + dx\n                    if 0 <= x_right < right.shape[1]:\n                        right_patch = right[y:y+9, x_right:x_right+9]\n\n                        # Sum squared differences (SSD)\n                        distance = np.sum((left_patch - right_patch)**2)\n\n                        if distance < best_distance:\n                            best_distance = distance\n                            best_disparity = abs(dx)\n\n                disparity[y, x] = best_disparity\n\n        return disparity\n'})}),"\n",(0,t.jsx)(n.h2,{id:"part-2-point-clouds",children:"Part 2: Point Clouds"}),"\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.strong,{children:"point cloud"})," is a 3D representation: a collection of (x, y, z) points in 3D space."]}),"\n",(0,t.jsx)(n.h3,{id:"point-cloud-representation",children:"Point Cloud Representation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PointCloud:\n    def __init__(self, points_xyz, colors_rgb=None):\n        """\n        Initialize point cloud\n\n        Args:\n            points_xyz: N\xd73 array of (x, y, z) coordinates\n            colors_rgb: N\xd73 array of (r, g, b) colors (optional)\n        """\n        self.points = points_xyz  # N\xd73\n        self.colors = colors_rgb  # N\xd73 or None\n        self.n_points = len(points_xyz)\n\n    def from_depth_image(self, depth_image, camera_intrinsics):\n        """Convert depth image to point cloud"""\n\n        height, width = depth_image.shape\n        fx, fy = camera_intrinsics[\'focal_x\'], camera_intrinsics[\'focal_y\']\n        cx, cy = camera_intrinsics[\'center_x\'], camera_intrinsics[\'center_y\']\n\n        points = []\n\n        for y in range(height):\n            for x in range(width):\n                z = depth_image[y, x]\n\n                if z > 0:  # Valid depth\n                    # Back-project pixel to 3D\n                    x_3d = (x - cx) * z / fx\n                    y_3d = (y - cy) * z / fy\n\n                    points.append([x_3d, y_3d, z])\n\n        return np.array(points)\n\n    def downsample_voxel(self, voxel_size=0.01):\n        """Reduce points by averaging in voxels"""\n\n        # Divide space into voxels\n        voxel_grid = {}\n\n        for point in self.points:\n            # Compute voxel key\n            voxel_key = tuple((point / voxel_size).astype(int))\n\n            if voxel_key not in voxel_grid:\n                voxel_grid[voxel_key] = []\n\n            voxel_grid[voxel_key].append(point)\n\n        # Average points in each voxel\n        downsampled = np.array([\n            np.mean(points, axis=0)\n            for points in voxel_grid.values()\n        ])\n\n        return downsampled\n\n    def remove_outliers_statistical(self, n_neighbors=20, std_ratio=2.0):\n        """Remove noise points using statistical outlier removal"""\n\n        from scipy.spatial import cKDTree\n\n        tree = cKDTree(self.points)\n        distances, indices = tree.query(self.points, k=n_neighbors+1)\n\n        # Compute mean distance for each point\n        mean_distances = np.mean(distances[:, 1:], axis=1)\n\n        # Compute global statistics\n        global_mean = np.mean(mean_distances)\n        global_std = np.std(mean_distances)\n\n        # Remove outliers\n        threshold = global_mean + std_ratio * global_std\n        inliers = mean_distances < threshold\n\n        return self.points[inliers]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"point-cloud-processing-pipeline",children:"Point Cloud Processing Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PointCloudProcessor:\n    def process_lidar_scan(self, raw_points):\n        """Process raw LIDAR points"""\n\n        # 1. Remove points beyond range (outdoors, reflections)\n        filtered = raw_points[np.linalg.norm(raw_points, axis=1) < 30]\n\n        # 2. Remove ground plane points\n        # (Assuming ground is roughly z=0)\n        ground_removal = filtered[np.abs(filtered[:, 2]) > 0.1]\n\n        # 3. Remove outliers\n        clean_points = self.remove_outliers_statistical(\n            ground_removal, n_neighbors=10, std_ratio=2.0\n        )\n\n        # 4. Downsample for efficiency\n        downsampled = self.downsample_voxel(clean_points, voxel_size=0.02)\n\n        return downsampled\n\n    def cluster_points(self, points, distance_threshold=0.05):\n        """Segment point cloud into clusters (objects)"""\n\n        from scipy.spatial.distance import pdist, squareform\n\n        # Compute pairwise distances\n        distances = squareform(pdist(points, metric=\'euclidean\'))\n\n        # DBSCAN clustering\n        clusters = self.dbscan(distances, eps=distance_threshold, min_samples=5)\n\n        return clusters\n\n    def dbscan(self, distances, eps, min_samples):\n        """Simple DBSCAN clustering"""\n\n        n_points = distances.shape[0]\n        labels = -1 * np.ones(n_points)\n        cluster_id = 0\n\n        for i in range(n_points):\n            if labels[i] != -1:\n                continue\n\n            neighbors = np.where(distances[i] <= eps)[0]\n\n            if len(neighbors) < min_samples:\n                labels[i] = -1  # Noise point\n                continue\n\n            # Start new cluster\n            labels[i] = cluster_id\n            queue = list(neighbors)\n\n            while queue:\n                j = queue.pop(0)\n\n                if labels[j] == -1:\n                    labels[j] = cluster_id\n\n                    neighbors_j = np.where(distances[j] <= eps)[0]\n                    if len(neighbors_j) >= min_samples:\n                        queue.extend(neighbors_j[labels[neighbors_j] == -1])\n\n            cluster_id += 1\n\n        return labels\n'})}),"\n",(0,t.jsx)(n.h2,{id:"part-3-3d-object-recognition",children:"Part 3: 3D Object Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Once you have a point cloud, you can recognize objects by shape."}),"\n",(0,t.jsx)(n.h3,{id:"iterative-closest-point-icp-alignment",children:"Iterative Closest Point (ICP) Alignment"}),"\n",(0,t.jsx)(n.p,{children:"ICP matches a point cloud to a 3D model by iteratively aligning them."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PointCloudMatcher:\n    def icp_align(self, source, target, max_iterations=50):\n        """Align source point cloud to target"""\n\n        source_current = np.copy(source)\n        total_transform = np.eye(4)\n\n        for iteration in range(max_iterations):\n            # 1. Find nearest point in target for each source point\n            from scipy.spatial import cKDTree\n            tree = cKDTree(target)\n            distances, indices = tree.query(source_current)\n\n            target_matched = target[indices]\n\n            # 2. Compute optimal rotation and translation\n            rotation, translation = self.compute_rigid_transform(\n                source_current, target_matched\n            )\n\n            # 3. Apply transform\n            transform = np.eye(4)\n            transform[:3, :3] = rotation\n            transform[:3, 3] = translation\n\n            source_current = (rotation @ source_current.T).T + translation\n            total_transform = transform @ total_transform\n\n            # 4. Check convergence\n            mean_error = np.mean(distances)\n            if mean_error < 0.001:\n                break\n\n        return source_current, total_transform, mean_error\n\n    def compute_rigid_transform(self, source, target):\n        """Compute optimal rotation and translation"""\n\n        # Center both point clouds\n        source_centered = source - np.mean(source, axis=0)\n        target_centered = target - np.mean(target, axis=0)\n\n        # Compute cross-covariance matrix\n        H = source_centered.T @ target_centered\n\n        # SVD to find rotation\n        U, S, Vt = np.linalg.svd(H)\n        rotation = Vt.T @ U.T\n\n        # Ensure proper rotation (det = 1)\n        if np.linalg.det(rotation) < 0:\n            Vt[-1, :] *= -1\n            rotation = Vt.T @ U.T\n\n        # Compute translation\n        translation = (np.mean(target, axis=0) -\n                      rotation @ np.mean(source, axis=0))\n\n        return rotation, translation\n'})}),"\n",(0,t.jsx)(n.h2,{id:"real-world-example-bin-picking",children:"Real-World Example: Bin Picking"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class BinPickingRobot:\n    def __init__(self):\n        self.depth_camera = DepthCamera()\n        self.point_processor = PointCloudProcessor()\n        self.arm = RobotArm()\n        self.gripper = Gripper()\n\n    def pick_from_bin(self):\n        \"\"\"Complete bin picking sequence\"\"\"\n\n        # 1. Acquire depth image\n        depth_image = self.depth_camera.capture()\n\n        # 2. Convert to point cloud\n        cloud = self.depth_camera.from_depth_image(depth_image)\n\n        # 3. Process: remove noise, ground, downsample\n        clean_cloud = self.point_processor.process_lidar_scan(cloud)\n\n        # 4. Cluster points to find individual objects\n        cluster_labels = self.point_processor.cluster_points(\n            clean_cloud, distance_threshold=0.05\n        )\n\n        # 5. For each cluster, compute grasp pose\n        best_grasp = None\n        best_score = 0\n\n        for cluster_id in np.unique(cluster_labels):\n            if cluster_id == -1:\n                continue\n\n            cluster_points = clean_cloud[cluster_labels == cluster_id]\n\n            # Compute centroid and approach direction\n            centroid = np.mean(cluster_points, axis=0)\n\n            # Try grasp from above\n            grasp_pose = {\n                'position': centroid + np.array([0, 0, 0.05]),\n                'orientation': np.eye(3),\n                'confidence': len(cluster_points) / 100  # More points = easier\n            }\n\n            if grasp_pose['confidence'] > best_score:\n                best_grasp = grasp_pose\n                best_score = grasp_pose['confidence']\n\n        if best_grasp is None:\n            return False\n\n        # 6. Execute grasp\n        self.arm.move_to(best_grasp['position'])\n        self.gripper.close()\n\n        return True\n"})}),"\n",(0,t.jsx)(n.h2,{id:"week-5-learning-outcomes",children:"Week 5 Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this week, you should be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explain"})," how depth cameras measure 3D structure"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Convert"})," depth images into point clouds"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Process"})," point clouds (downsampling, outlier removal, clustering)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," ICP for aligning point clouds"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recognize"})," 3D objects by comparing point cloud shapes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Apply"})," 3D perception to robotic bin picking"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Term"}),(0,t.jsx)(n.th,{children:"Definition"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Depth image"})}),(0,t.jsx)(n.td,{children:"2D array where each pixel contains distance to surface"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Point cloud"})}),(0,t.jsx)(n.td,{children:"3D representation as collection of (x, y, z) points"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Disparity"})}),(0,t.jsx)(n.td,{children:"Difference in pixel position between stereo images"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Voxel"})}),(0,t.jsx)(n.td,{children:"3D grid cell (volumetric pixel)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Outlier"})}),(0,t.jsx)(n.td,{children:"Noise point that doesn't fit with other points"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Cluster"})}),(0,t.jsx)(n.td,{children:"Group of connected points representing single object"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"ICP"})}),(0,t.jsx)(n.td,{children:"Iterative Closest Point - algorithm for cloud alignment"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Occlusion"})}),(0,t.jsx)(n.td,{children:"Object hidden behind another object"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"discussion-questions",children:"Discussion Questions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sensor selection"}),": For a warehouse robot picking boxes, would you use stereo vision, LIDAR, or RGB-D cameras? Why?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Computational load"}),": Converting a 640\xd7480 depth image to point cloud requires ~300K 3D transformations per frame. At 30 FPS, that's 9M calculations/second. How would you optimize?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Point cloud clustering can fail when objects touch. How would you handle densely packed bins?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Multi-sensor fusion"}),": Why fuse LIDAR + stereo? What does each sensor contribute?"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-activity",children:"Hands-On Activity"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Process a simulated point cloud"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Generate synthetic 3D data (download KITTI dataset or generate noise)"}),"\n",(0,t.jsxs)(n.li,{children:["Implement point cloud pipeline:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Downsample voxel grid"}),"\n",(0,t.jsx)(n.li,{children:"Remove statistical outliers"}),"\n",(0,t.jsx)(n.li,{children:"Cluster points"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Visualize results using Open3D or PyVista"}),"\n",(0,t.jsx)(n.li,{children:"Measure clustering accuracy (precision/recall)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"resources-for-deeper-learning",children:"Resources for Deeper Learning"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Library"}),": Open3D - Python 3D data processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Library"}),": Point Cloud Library (PCL) - C++ point cloud processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dataset"}),": KITTI dataset - autonomous driving with 3D data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Course"}),': "3D Vision" - UC Berkeley (YouTube available)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Paper"}),': "A Method for Registration of 3-D Shapes" - ICP algorithm']}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next"}),": Week 6 - SLAM & Localization"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,t.jsx)(n.strong,{children:"Tip"}),": Visualizing point clouds is essential! Use tools like CloudCompare or Open3D's visualizer to understand your data."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:function(e,n,i){i.d(n,{R:function(){return o},x:function(){return l}});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);