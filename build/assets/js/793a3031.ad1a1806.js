"use strict";(self.webpackChunkai_textbook_frontend=self.webpackChunkai_textbook_frontend||[]).push([[98],{805:function(n,e,t){t.r(e),t.d(e,{assets:function(){return l},contentTitle:function(){return o},default:function(){return g},frontMatter:function(){return a},metadata:function(){return r},toc:function(){return d}});var r=JSON.parse('{"id":"integration/week-10-learning","title":"Week 10: Learning from Data & Imitation Learning","description":"From Programming to Learning","source":"@site/docs/04-integration/week-10-learning.mdx","sourceDirName":"04-integration","slug":"/integration/week-10-learning","permalink":"/ai-textbook/docs/integration/week-10-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-textbook/tree/main/docs/04-integration/week-10-learning.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Week 9: Mobile Robot Navigation","permalink":"/ai-textbook/docs/control/week-9-mobile-navigation"},"next":{"title":"Week 11: System Integration & Real-World Deployment","permalink":"/ai-textbook/docs/integration/week-11-deployment"}}'),i=t(4848),s=t(8453);const a={sidebar_position:1},o="Week 10: Learning from Data & Imitation Learning",l={},d=[{value:"From Programming to Learning",id:"from-programming-to-learning",level:2},{value:"Part 1: Behavior Cloning",id:"part-1-behavior-cloning",level:2},{value:"Part 2: DAgger (Dataset Aggregation)",id:"part-2-dagger-dataset-aggregation",level:2},{value:"Part 3: Learning for Manipulation",id:"part-3-learning-for-manipulation",level:2},{value:"Week 10 Learning Outcomes",id:"week-10-learning-outcomes",level:2},{value:"Key Terminology",id:"key-terminology",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"week-10-learning-from-data--imitation-learning",children:"Week 10: Learning from Data & Imitation Learning"})}),"\n",(0,i.jsx)(e.h2,{id:"from-programming-to-learning",children:"From Programming to Learning"}),"\n",(0,i.jsx)(e.p,{children:"So far we've programmed robots explicitly: write control laws, planning algorithms, vision pipelines. But many real-world tasks are too complex to program by hand."}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Machine learning"})," enables robots to learn from data. This week focuses on ",(0,i.jsx)(e.strong,{children:"imitation learning"}),": learning by observing humans."]}),"\n",(0,i.jsx)(e.h2,{id:"part-1-behavior-cloning",children:"Part 1: Behavior Cloning"}),"\n",(0,i.jsx)(e.p,{children:"Simplest form of imitation learning: supervised learning on human demonstrations."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class BehaviorCloner:\n    def __init__(self, state_dim, action_dim):\n        """Initialize behavior cloning model"""\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n\n        # Simple neural network: state \u2192 action\n        self.model = self.build_model()\n\n    def build_model(self):\n        """Build neural network"""\n        # Simplified; real implementations use PyTorch/TensorFlow\n\n        class SimpleNet:\n            def __init__(self, input_dim, output_dim):\n                self.w1 = np.random.randn(input_dim, 128) * 0.01\n                self.b1 = np.zeros(128)\n                self.w2 = np.random.randn(128, output_dim) * 0.01\n                self.b2 = np.zeros(output_dim)\n\n            def forward(self, x):\n                # ReLU hidden layer\n                h = np.maximum(0, x @ self.w1 + self.b1)\n                # Linear output\n                y = h @ self.w2 + self.b2\n                return y\n\n        return SimpleNet(self.state_dim, self.action_dim)\n\n    def train(self, states, actions, epochs=100, learning_rate=0.001):\n        """Train on human demonstrations"""\n\n        for epoch in range(epochs):\n            # Batch gradient descent\n            predictions = self.model.forward(states)\n\n            # Mean squared error loss\n            loss = np.mean((predictions - actions)**2)\n\n            # Backprop (simplified)\n            gradient = 2 * (predictions - actions) / len(states)\n\n            # Update weights\n            # (Real implementations use automatic differentiation)\n\n            if epoch % 10 == 0:\n                print(f"Epoch {epoch}: Loss = {loss:.4f}")\n\n    def predict(self, state):\n        """Predict action for given state"""\n        return self.model.forward(state)\n'})}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": ",(0,i.jsx)(e.strong,{children:"Distribution shift"}),". Robot might encounter states not in training data."]}),"\n",(0,i.jsx)(e.h2,{id:"part-2-dagger-dataset-aggregation",children:"Part 2: DAgger (Dataset Aggregation)"}),"\n",(0,i.jsx)(e.p,{children:"Iteratively collect failures and have human correct them."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class DAggerTrainer:\n    def __init__(self, behavior_cloner, environment, expert_policy):\n        self.cloner = behavior_cloner\n        self.env = environment\n        self.expert = expert_policy\n        self.aggregate_states = []\n        self.aggregate_actions = []\n\n    def train_with_dagger(self, num_iterations=10, num_rollouts=5):\n        """DAgger algorithm"""\n\n        for iteration in range(num_iterations):\n            print(f"\\\\nDAgger iteration {iteration+1}/{num_iterations}")\n\n            # 1. Collect rollouts with current policy\n            for rollout in range(num_rollouts):\n                state = self.env.reset()\n                rollout_states = []\n\n                for step in range(100):\n                    # Use learned policy\n                    action = self.cloner.predict(state)\n\n                    rollout_states.append(state)\n                    state, reward, done = self.env.step(action)\n\n                    if done:\n                        break\n\n                # 2. Ask expert to label trajectory\n                # In real systems: human watches and corrects\n\n                # 3. Add to aggregate dataset\n                for s in rollout_states:\n                    expert_action = self.expert.get_action(s)\n                    self.aggregate_states.append(s)\n                    self.aggregate_actions.append(expert_action)\n\n            # 4. Retrain on all data collected so far\n            self.cloner.train(\n                np.array(self.aggregate_states),\n                np.array(self.aggregate_actions),\n                epochs=50\n            )\n\n            # Evaluate\n            success_rate = self.evaluate()\n            print(f"Success rate: {success_rate:.1%}")\n\n    def evaluate(self, num_episodes=10):\n        """Evaluate learned policy"""\n        successes = 0\n\n        for episode in range(num_episodes):\n            state = self.env.reset()\n\n            for step in range(100):\n                action = self.cloner.predict(state)\n                state, reward, done = self.env.step(action)\n\n                if done and reward > 0:  # Successful completion\n                    successes += 1\n                    break\n                if done:\n                    break\n\n        return successes / num_episodes\n'})}),"\n",(0,i.jsx)(e.h2,{id:"part-3-learning-for-manipulation",children:"Part 3: Learning for Manipulation"}),"\n",(0,i.jsx)(e.p,{children:"Real example: learning to grasp from demonstrations."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class GraspingLearner:\n    def __init__(self):\n        self.demo_states = []  # RGB images\n        self.demo_actions = []  # Grasp positions\n\n    def collect_demonstrations(self, num_demos=100):\n        """Collect human grasping demonstrations"""\n\n        for demo in range(num_demos):\n            # Show object from multiple angles\n            rgb_images = self.capture_from_angles(num_angles=6)\n\n            # Ask human: where to grasp?\n            # In real system: human teleoperates robot\n\n            for image in rgb_images:\n                self.demo_states.append(image)\n                # Grasp location: (x, y, angle)\n                grasp = self.get_human_grasp()\n                self.demo_actions.append(grasp)\n\n    def train_grasp_network(self):\n        """Train CNN to predict grasp location"""\n\n        # Input: RGB image\n        # Output: Heatmap of grasp quality\n\n        model = self.build_grasp_cnn()\n\n        # Train on demonstrations\n        model.train(self.demo_states, self.demo_actions, epochs=100)\n\n        return model\n\n    def predict_grasps(self, rgb_image, num_grasps=5):\n        """Predict top 5 grasp candidates"""\n\n        model = self.trained_model\n\n        # Get heatmap of grasp quality\n        heatmap = model.predict(rgb_image)\n\n        # Find peaks\n        grasps = self.find_peaks(heatmap, k=num_grasps)\n\n        return grasps\n\n    def build_grasp_cnn(self):\n        """Build CNN for grasp prediction"""\n\n        # Input: 224\xd7224 RGB image\n        # \u2192 Conv layers extract features\n        # \u2192 Output: 28\xd728 heatmap of grasp quality\n\n        class GraspCNN:\n            def __init__(self):\n                # Simplified CNN structure\n                pass\n\n            def train(self, images, actions, epochs):\n                # Training loop\n                pass\n\n            def predict(self, image):\n                # Forward pass\n                pass\n\n        return GraspCNN()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"week-10-learning-outcomes",children:"Week 10 Learning Outcomes"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this week, you should be able to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Explain"})," behavior cloning and distribution shift problem"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Implement"})," DAgger for iterative learning"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Train"})," neural networks on robot demonstrations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Apply"})," imitation learning to manipulation tasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Evaluate"})," learned policies and diagnose failure modes"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Understand"})," limitations of supervised learning for robotics"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Term"}),(0,i.jsx)(e.th,{children:"Definition"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Imitation learning"})}),(0,i.jsx)(e.td,{children:"Learning by observing and copying expert behavior"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Behavior cloning"})}),(0,i.jsx)(e.td,{children:"Supervised learning: state \u2192 action from demonstrations"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Distribution shift"})}),(0,i.jsx)(e.td,{children:"Robot encounters states not in training data"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"DAgger"})}),(0,i.jsx)(e.td,{children:"Dataset Aggregation - iterative correction from expert"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Teleoperati"})}),(0,i.jsx)(e.td,{children:"Remote control by human (used for data collection)"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Trajectory"})}),(0,i.jsx)(e.td,{children:"Sequence of (state, action, reward) tuples"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Heatmap"})}),(0,i.jsx)(e.td,{children:"2D visualization of predicted values"})]})]})]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Next"}),": Week 11 - System Integration & Real-World Deployment"]}),"\n",(0,i.jsxs)(e.p,{children:["\ud83d\udca1 ",(0,i.jsx)(e.strong,{children:"Tip"}),": DAgger is more robust than behavior cloning because it addresses distribution shift!"]})]})}function g(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(c,{...n})}):c(n)}},8453:function(n,e,t){t.d(e,{R:function(){return a},x:function(){return o}});var r=t(6540);const i={},s=r.createContext(i);function a(n){const e=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);