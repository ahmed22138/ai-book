"use strict";(self.webpackChunkai_textbook_frontend=self.webpackChunkai_textbook_frontend||[]).push([[799],{6516:function(e,n,r){r.r(n),r.d(n,{assets:function(){return c},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return o},metadata:function(){return i},toc:function(){return a}});var i=JSON.parse('{"id":"introduction/week-2-robot-anatomy","title":"Week 2: Robot Anatomy & Sensors","description":"Introduction to Robot Hardware","source":"@site/docs/01-introduction/week-2-robot-anatomy.mdx","sourceDirName":"01-introduction","slug":"/introduction/week-2-robot-anatomy","permalink":"/ai-textbook/docs/introduction/week-2-robot-anatomy","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-textbook/tree/main/docs/01-introduction/week-2-robot-anatomy.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Week 1: Embodied AI Fundamentals","permalink":"/ai-textbook/docs/introduction/week-1-embodied-ai"},"next":{"title":"Week 3: Control Systems Basics","permalink":"/ai-textbook/docs/introduction/week-3-control-systems"}}'),s=r(4848),t=r(8453);const o={sidebar_position:2},l="Week 2: Robot Anatomy & Sensors",c={},a=[{value:"Introduction to Robot Hardware",id:"introduction-to-robot-hardware",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:3},{value:"Part 1: Robotic Kinematics &amp; Structure",id:"part-1-robotic-kinematics--structure",level:2},{value:"Degrees of Freedom (DOF)",id:"degrees-of-freedom-dof",level:3},{value:"Common Robot Morphologies",id:"common-robot-morphologies",level:3},{value:"Kinematic Chains",id:"kinematic-chains",level:3},{value:"Workspace Analysis",id:"workspace-analysis",level:3},{value:"Part 2: Robot Sensors",id:"part-2-robot-sensors",level:2},{value:"Vision Systems",id:"vision-systems",level:3},{value:"Tactile Sensors",id:"tactile-sensors",level:3},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:3},{value:"LIDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:3},{value:"Part 3: Robot Actuators",id:"part-3-robot-actuators",level:2},{value:"Motor Types",id:"motor-types",level:3},{value:"Gripper Types",id:"gripper-types",level:3},{value:"Control Architecture",id:"control-architecture",level:3},{value:"Real-World Example: Picking a Bottle",id:"real-world-example-picking-a-bottle",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Week 2 Learning Outcomes",id:"week-2-learning-outcomes",level:2},{value:"Key Terminology",id:"key-terminology",level:2},{value:"Discussion Questions",id:"discussion-questions",level:2},{value:"Hands-On Activity",id:"hands-on-activity",level:2},{value:"Resources for Deeper Learning",id:"resources-for-deeper-learning",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-2-robot-anatomy--sensors",children:"Week 2: Robot Anatomy & Sensors"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-robot-hardware",children:"Introduction to Robot Hardware"}),"\n",(0,s.jsx)(n.p,{children:"Welcome to Week 2! Last week, we explored the philosophical foundations of embodied AI. This week, we ground ourselves in the physical reality of robots\u2014the hardware systems that give embodied AI systems their power to perceive and act."}),"\n",(0,s.jsx)(n.h3,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,s.jsx)(n.p,{children:"In this lesson, we'll examine:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robotic kinematics"}),": How robots move and position themselves"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor types"}),": The eyes, ears, and touch receptors of robots"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Actuators"}),": The muscles that make robots move"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-world robot architectures"}),": Complete systems built from these components"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"part-1-robotic-kinematics--structure",children:"Part 1: Robotic Kinematics & Structure"}),"\n",(0,s.jsx)(n.h3,{id:"degrees-of-freedom-dof",children:"Degrees of Freedom (DOF)"}),"\n",(0,s.jsxs)(n.p,{children:["A robot's ",(0,s.jsx)(n.strong,{children:"degree of freedom"})," is each independent way it can move. Understanding DOF is critical for understanding what tasks a robot can accomplish."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: A robot arm picking up a water bottle"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Joint 1 (Base rotation)     \u2192 Rotate around vertical axis\nJoint 2 (Shoulder)          \u2192 Move arm up/down\nJoint 3 (Elbow)             \u2192 Bend/extend forearm\nJoint 4 (Wrist rotation)    \u2192 Twist wrist\nJoint 5 (Wrist bend)        \u2192 Tilt wrist up/down\nJoint 6 (Gripper rotation)  \u2192 Rotate end effector\nGripper (End effector)      \u2192 Open/close fingers\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This arm has ",(0,s.jsx)(n.strong,{children:"7 DOF"})," (6 joints + gripper), making it highly dexterous."]}),"\n",(0,s.jsx)(n.h3,{id:"common-robot-morphologies",children:"Common Robot Morphologies"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Robot Type"}),(0,s.jsx)(n.th,{children:"DOF"}),(0,s.jsx)(n.th,{children:"Example"}),(0,s.jsx)(n.th,{children:"Common Tasks"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Industrial Arm"})}),(0,s.jsx)(n.td,{children:"6-7"}),(0,s.jsx)(n.td,{children:"FANUC CRX"}),(0,s.jsx)(n.td,{children:"Assembly, welding, palletizing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Mobile Manipulator"})}),(0,s.jsx)(n.td,{children:"7-12"}),(0,s.jsx)(n.td,{children:"Boston Dynamics Stretch"}),(0,s.jsx)(n.td,{children:"Bin picking, shelf organization"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Humanoid"})}),(0,s.jsx)(n.td,{children:"20-40"}),(0,s.jsx)(n.td,{children:"Tesla Optimus"}),(0,s.jsx)(n.td,{children:"General manipulation, navigation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Quadruped"})}),(0,s.jsx)(n.td,{children:"12-16"}),(0,s.jsx)(n.td,{children:"Boston Dynamics Spot"}),(0,s.jsx)(n.td,{children:"Inspection, rough terrain, mapping"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Aerial (Drone)"})}),(0,s.jsx)(n.td,{children:"4-6"}),(0,s.jsx)(n.td,{children:"DJI M300"}),(0,s.jsx)(n.td,{children:"Inspection, delivery, surveillance"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"kinematic-chains",children:"Kinematic Chains"}),"\n",(0,s.jsxs)(n.p,{children:["A robot's joints form a ",(0,s.jsx)(n.strong,{children:"kinematic chain"}),"\u2014a sequence of connected links. Two main types exist:"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"1. Serial Chains"})," (most common in manipulators)"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Base \u2190 Joint1 \u2190 Link1 \u2190 Joint2 \u2190 Link2 \u2190 ... \u2190 End Effector\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Advantages: Simple control, long reach"}),"\n",(0,s.jsx)(n.li,{children:"Disadvantages: Less stiff, can only reach positions in workspace"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"2. Parallel Chains"})," (found in specialized systems)"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"      \u2190 Joint1 \u2190\nBase \u2190  Joint2  \u2190 End Effector\n      \u2190 Joint3 \u2190\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Advantages: Higher stiffness, faster"}),"\n",(0,s.jsx)(n.li,{children:"Disadvantages: Limited workspace, complex control"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"workspace-analysis",children:"Workspace Analysis"}),"\n",(0,s.jsxs)(n.p,{children:["Every robot has a ",(0,s.jsx)(n.strong,{children:"workspace"}),"\u2014the region in 3D space its end effector can reach."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Pseudo-code: Computing robot workspace\nclass RobotArm:\n    def compute_workspace(self, joint_limits):\n        """\n        Calculate reachable positions by sampling joint configurations\n        """\n        workspace_points = []\n\n        # Sample all joint angle combinations\n        for theta1 in range(-90, 90, 5):\n            for theta2 in range(-45, 135, 5):\n                for theta3 in range(-90, 90, 5):\n                    # Forward kinematics: calculate end-effector position\n                    end_effector = self.forward_kinematics(\n                        [theta1, theta2, theta3]\n                    )\n                    workspace_points.append(end_effector)\n\n        return workspace_points\n\n    def forward_kinematics(self, joint_angles):\n        """Calculate end-effector position from joint angles"""\n        # Use Denavit-Hartenberg parameters to compute positions\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"part-2-robot-sensors",children:"Part 2: Robot Sensors"}),"\n",(0,s.jsx)(n.p,{children:"Sensors are how robots perceive their world. They convert physical phenomena into digital signals."}),"\n",(0,s.jsx)(n.h3,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB Cameras"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Capture color images at 30-60 FPS"}),"\n",(0,s.jsx)(n.li,{children:"Used for object detection and scene understanding"}),"\n",(0,s.jsx)(n.li,{children:"Passive (no active illumination needed)"}),"\n",(0,s.jsx)(n.li,{children:"Challenge: Doesn't directly measure depth"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Depth Cameras"})," (RGB-D)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Measure distance to objects using infrared or time-of-flight (ToF)"}),"\n",(0,s.jsx)(n.li,{children:"Return 3D point clouds (~30,000 points per frame)"}),"\n",(0,s.jsx)(n.li,{children:"Examples: Intel RealSense, Azure Kinect"}),"\n",(0,s.jsx)(n.li,{children:"Challenge: Struggles with reflective or transparent surfaces"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Object Detection from RGB-D"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VisionSystem:\n    def detect_bottle(self, rgb_image, depth_image):\n        """Detect water bottle in scene"""\n\n        # 1. Convert RGB to HSV for color-based segmentation\n        hsv = convert_rgb_to_hsv(rgb_image)\n\n        # 2. Segment blue pixels (water bottle color)\n        bottle_mask = extract_color_range(hsv,\n            hue_min=200, hue_max=240,  # Blue range\n            sat_min=50, sat_max=255,\n            val_min=50, val_max=255\n        )\n\n        # 3. Find contours in mask\n        contours = find_contours(bottle_mask)\n\n        # 4. For each contour, extract 3D position from depth\n        bottles_in_3d = []\n        for contour in contours:\n            # Get depth values within contour\n            depths = depth_image[contour]\n            avg_depth = mean(depths)\n\n            # Convert pixel coordinates to 3D world position\n            x, y = get_centroid(contour)\n            position_3d = pixel_to_world(x, y, avg_depth)\n\n            bottles_in_3d.append({\n                "position": position_3d,\n                "confidence": calculate_confidence(contour),\n                "size": get_bounding_box(contour)\n            })\n\n        return bottles_in_3d\n'})}),"\n",(0,s.jsx)(n.h3,{id:"tactile-sensors",children:"Tactile Sensors"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Force/Torque Sensors"})," (F/T sensors)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Measure forces (Fx, Fy, Fz) and torques (Rx, Ry, Rz)"}),"\n",(0,s.jsx)(n.li,{children:"Typically mounted at wrist or gripper"}),"\n",(0,s.jsxs)(n.li,{children:["Enable ",(0,s.jsx)(n.strong,{children:"compliant manipulation"})," (soft, adaptive grasping)"]}),"\n",(0,s.jsx)(n.li,{children:"Sampling rate: 100-1000 Hz"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Tactile Array Sensors"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multiple pressure sensors on gripper fingers"}),"\n",(0,s.jsx)(n.li,{children:"Detect contact pressure distribution"}),"\n",(0,s.jsx)(n.li,{children:"Help determine if object is slipping"}),"\n",(0,s.jsx)(n.li,{children:"Used for in-hand manipulation"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Tactile Sensor Application: Delicate Grasping"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class GripperController:\n    def grasp_fragile_object(self, target_force=5.0):\n        """Grasp object while monitoring force feedback"""\n\n        # Start with open gripper\n        self.gripper.open()\n\n        while True:\n            # Read current force\n            current_force = self.force_sensor.read_force()\n\n            # Check for slipping (force increase without gripper closing)\n            if self.force_sensor.is_slipping():\n                # Object slipping! Close gripper slightly more\n                self.gripper.close(increment=0.1)\n                continue\n\n            # Check for successful grasp\n            if current_force >= target_force:\n                # Successfully grasped\n                return True\n\n            # Not yet sufficient force - close gripper more\n            self.gripper.close(increment=0.2)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Joint Encoders"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Measure the angle of each joint"}),"\n",(0,s.jsx)(n.li,{children:"Absolute: Report actual position (even after power-off)"}),"\n",(0,s.jsx)(n.li,{children:"Incremental: Track changes from reference position"}),"\n",(0,s.jsx)(n.li,{children:"Resolution: 12-16 bits typically"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inertial Measurement Unit (IMU)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Measures acceleration (accelerometer: 3 axes)"}),"\n",(0,s.jsx)(n.li,{children:"Measures rotation rate (gyroscope: 3 axes)"}),"\n",(0,s.jsx)(n.li,{children:"Measures magnetic field (magnetometer: 3 axes)"}),"\n",(0,s.jsx)(n.li,{children:"Critical for mobile robots and humanoids to estimate orientation"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"IMU in Mobile Robot Navigation"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MobileRobotIMU:\n    def estimate_orientation(self):\n        """Use IMU to estimate robot\'s current orientation"""\n\n        # Read sensor values\n        accel = self.imu.read_accelerometer()  # [x, y, z] m/s\xb2\n        gyro = self.imu.read_gyroscope()       # [x, y, z] rad/s\n\n        # Accelerometer gives gravity direction (down)\n        # Use it to estimate pitch and roll\n        pitch = atan2(accel.x, sqrt(accel.y\xb2 + accel.z\xb2))\n        roll = atan2(accel.y, sqrt(accel.x\xb2 + accel.z\xb2))\n\n        # Gyroscope gives rotation rates\n        # Integrate to get yaw (heading)\n        self.yaw += gyro.z * dt\n\n        # Combine for robust orientation\n        orientation = {\n            "roll": roll,\n            "pitch": pitch,\n            "yaw": self.yaw\n        }\n\n        return orientation\n'})}),"\n",(0,s.jsx)(n.h3,{id:"lidar-light-detection-and-ranging",children:"LIDAR (Light Detection and Ranging)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Active laser scanning to measure distances"}),"\n",(0,s.jsx)(n.li,{children:"Returns point cloud of entire environment"}),"\n",(0,s.jsx)(n.li,{children:"High precision and range (up to 200m)"}),"\n",(0,s.jsx)(n.li,{children:"Mostly unaffected by lighting conditions"}),"\n",(0,s.jsx)(n.li,{children:"Used for SLAM (Simultaneous Localization and Mapping)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Advantages over RGB-D:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Longer range"}),"\n",(0,s.jsx)(n.li,{children:"Works in bright sunlight"}),"\n",(0,s.jsx)(n.li,{children:"Better for outdoors"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LIDAR Application: Obstacle Detection"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class LidarObstacleDetector:\n    def detect_obstacles(self, lidar_scan):\n        """Find obstacles from LIDAR point cloud"""\n\n        obstacles = []\n\n        for point in lidar_scan.points:\n            x, y, z = point.position\n\n            # Check if point is at robot height (z \u2248 0.5m)\n            if 0.2 < z < 1.0:\n                # Convert to grid cell\n                grid_x = int(x / 0.1)  # 10cm resolution\n                grid_y = int(y / 0.1)\n\n                # Mark as occupied\n                self.occupancy_grid[grid_x, grid_y] = 1\n\n                # If cluster of occupied cells, it\'s an obstacle\n                if self.is_large_cluster(grid_x, grid_y):\n                    obstacle = {\n                        "position": point.position,\n                        "distance": sqrt(x\xb2 + y\xb2),\n                        "size": self.estimate_size(grid_x, grid_y)\n                    }\n                    obstacles.append(obstacle)\n\n        return obstacles\n'})}),"\n",(0,s.jsx)(n.h2,{id:"part-3-robot-actuators",children:"Part 3: Robot Actuators"}),"\n",(0,s.jsx)(n.p,{children:"Actuators convert electrical signals into physical motion."}),"\n",(0,s.jsx)(n.h3,{id:"motor-types",children:"Motor Types"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Motor Type"}),(0,s.jsx)(n.th,{children:"Torque Range"}),(0,s.jsx)(n.th,{children:"Speed"}),(0,s.jsx)(n.th,{children:"Efficiency"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"DC Motor"})}),(0,s.jsx)(n.td,{children:"Low-Medium"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"80-90%"}),(0,s.jsx)(n.td,{children:"Wheels, simple joints"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Servo Motor"})}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"70-80%"}),(0,s.jsx)(n.td,{children:"Precise positioning"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Stepper Motor"})}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Low-Medium"}),(0,s.jsx)(n.td,{children:"50-70%"}),(0,s.jsx)(n.td,{children:"Accurate steps, 3D printers"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"BLDC Motor"})}),(0,s.jsx)(n.td,{children:"Medium-High"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"85-95%"}),(0,s.jsx)(n.td,{children:"Drones, fast joints"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Harmonic Drive"})}),(0,s.jsx)(n.td,{children:"Very High"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"75-85%"}),(0,s.jsx)(n.td,{children:"Robot arms, precision"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"gripper-types",children:"Gripper Types"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parallel Jaw Gripper"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Finger \u2190\u2015\u2015\u2015\u2192 Finger\n        Object\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simple, fast (0.5-2 sec close)"}),"\n",(0,s.jsx)(n.li,{children:"Limited grasp strategies"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Box picking, bin picking"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Adaptive Gripper"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"   \u2571\u2500\u2572\n  \u2571   \u2572    Fingers conform to object shape\n \u2571     \u2572\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fingers adjust to object shape"}),"\n",(0,s.jsx)(n.li,{children:"Slower but more robust"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Unknown objects, delicate items"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Suction Gripper"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"  \u2554\u2550\u2550\u2550\u2550\u2557\n  \u2551Pump\u2551\u2192 Vacuum\n  \u255a\u2550\u2550\u2550\u2550\u255d\n\n  \u2571\u2500\u2500\u2500\u2500\u2500\u2572\n \u2502Suction \u2502 Creates adhesion\n \u2502 Cups   \u2502\n  \u2572\u2500\u2500\u2500\u2500\u2500\u2571\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fast, gentle"}),"\n",(0,s.jsx)(n.li,{children:"Works on smooth surfaces"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Flat objects, food, boxes"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"control-architecture",children:"Control Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   High-Level Planning Layer      \u2502\n\u2502  (Goal: Pick up water bottle)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Mid-Level Motion Planning      \u2502\n\u2502  (Generate arm trajectory)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Low-Level Joint Control (PID)   \u2502\n\u2502  (Command motor speeds/torques)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Motor Drivers & Hardware       \u2502\n\u2502  (Power electronics)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-world-example-picking-a-bottle",children:"Real-World Example: Picking a Bottle"}),"\n",(0,s.jsx)(n.p,{children:"Let's tie everything together with a complete picking scenario:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class BottlePickingRobot:\n    def __init__(self):\n        self.vision = VisionSystem()\n        self.arm = RobotArm()\n        self.gripper = Gripper()\n        self.force_sensor = ForceSensor()\n\n    def pick_bottle(self):\n        """Complete picking sequence"""\n\n        # 1. SENSE: Perceive bottle location\n        print("\ud83d\udcf7 Capturing image...")\n        rgb, depth = self.vision.capture()\n        bottles = self.vision.detect_bottle(rgb, depth)\n\n        if not bottles:\n            return False\n\n        target = bottles[0]  # Pick closest bottle\n        print(f"\ud83d\udd0d Found bottle at {target[\'position\']}")\n\n        # 2. THINK: Plan trajectory to bottle\n        print("\ud83e\udd16 Planning trajectory...")\n        approach_pose = self._compute_approach_pose(target)\n        grasp_pose = self._compute_grasp_pose(target)\n\n        # 3. ACT: Move arm to bottle\n        print("\u27a1\ufe0f  Moving to approach pose...")\n        self.arm.move_to(approach_pose, duration=2.0)\n\n        # 4. SENSE: Verify we\'re close\n        current_depth = self.vision.get_depth_at_center()\n        if abs(current_depth - target[\'distance\']) > 0.05:\n            print("\u26a0\ufe0f  Distance mismatch - recalibrating...")\n            target = self.vision.detect_bottle(rgb, depth)[0]\n\n        # 5. ACT: Move to grasp pose\n        print("\u27a1\ufe0f  Moving to grasp pose...")\n        self.arm.move_to(grasp_pose, duration=1.0)\n\n        # 6. ACT: Close gripper\n        print("\u270b Grasping...")\n        success = self.gripper.grasp_fragile_object(target_force=5.0)\n\n        if not success:\n            print("\u274c Grasp failed - object too small or slipped")\n            self.gripper.open()\n            return False\n\n        # 7. SENSE: Verify grasp with force sensor\n        grasp_force = self.force_sensor.read_force()\n        print(f"\u2705 Grasped with force: {grasp_force} N")\n\n        # 8. ACT: Lift bottle\n        print("\u2b06\ufe0f  Lifting...")\n        self.arm.move_to(grasp_pose + [0, 0, 0.3], duration=1.0)\n\n        return True\n\n    def _compute_approach_pose(self, target):\n        """Compute pose 15cm above bottle"""\n        return target[\'position\'] + [0, 0, 0.15]\n\n    def _compute_grasp_pose(self, target):\n        """Compute pose for grasping bottle"""\n        return target[\'position\']\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Real robots don't rely on a single sensor. They combine multiple sensor inputs for robust perception."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SensorFusion:\n    def estimate_robot_state(self):\n        """Combine multiple sensors for robust state estimate"""\n\n        # From IMU: rough orientation\n        imu_yaw = self.imu.estimate_yaw()\n\n        # From wheel encoders: rough distance traveled\n        encoder_x, encoder_y = self.encoders.get_displacement()\n\n        # From LIDAR: precise position relative to known map\n        lidar_pose = self.lidar_localization.get_pose()\n\n        # Fuse with weights: LIDAR most reliable, IMU supplements\n        fused_pose = {\n            "x": 0.8 * lidar_pose.x + 0.2 * encoder_x,\n            "y": 0.8 * lidar_pose.y + 0.2 * encoder_y,\n            "theta": 0.7 * lidar_pose.theta + 0.3 * imu_yaw\n        }\n\n        return fused_pose\n'})}),"\n",(0,s.jsx)(n.h2,{id:"week-2-learning-outcomes",children:"Week 2 Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this week, you should be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Define"})," degrees of freedom and explain how they constrain robot capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calculate"})," the forward kinematics for a simple 2-DOF robot arm"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compare"})," different sensor types and explain when to use each (RGB-D vs LIDAR vs IMU)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design"})," a sensor suite for a specific robotic task (e.g., bin picking, navigation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," basic sensor fusion to combine multiple sensor inputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Analyze"})," a robot architecture and identify sensing and actuation bottlenecks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Term"}),(0,s.jsx)(n.th,{children:"Definition"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Degree of Freedom (DOF)"})}),(0,s.jsx)(n.td,{children:"Independent way a robot can move; each joint contributes 1 DOF"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"End Effector"})}),(0,s.jsx)(n.td,{children:"Tool at the end of robot arm (gripper, sensor, etc.)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Workspace"})}),(0,s.jsx)(n.td,{children:"All positions reachable by robot's end effector"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Kinematics"})}),(0,s.jsx)(n.td,{children:"Study of motion without considering forces"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Point Cloud"})}),(0,s.jsx)(n.td,{children:"3D data represented as collection of points (x, y, z)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"SLAM"})}),(0,s.jsx)(n.td,{children:"Simultaneous Localization and Mapping\u2014navigating while building a map"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Actuator"})}),(0,s.jsx)(n.td,{children:"Device that converts electrical signal to mechanical motion"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Sensor Fusion"})}),(0,s.jsx)(n.td,{children:"Combining multiple sensor inputs for robust estimates"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"discussion-questions",children:"Discussion Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tradeoffs"}),": Why might an engineer choose a 6-DOF arm for a task instead of a 7-DOF arm? What's the advantage of fewer joints?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Sensor Selection"}),": You're designing a robot to pick ripe tomatoes in a greenhouse. What sensors would you use and why? What about picking boxes in a warehouse?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-time Constraints"}),": A robot's sensor captures depth images at 30 Hz. Why might processing all images in real-time be impossible? What strategies could help?"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Graceful Degradation"}),": If your LIDAR fails, how could your robot continue moving safely using only wheel encoders and IMU?"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-activity",children:"Hands-On Activity"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Reverse Engineering a Robot"})}),"\n",(0,s.jsx)(n.p,{children:"Find a YouTube video of an industrial robot (e.g., FANUC, ABB, KUKA) performing a task. Watch carefully and:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Count the number of joints (DOF)"}),"\n",(0,s.jsx)(n.li,{children:"Identify the sensors you can observe"}),"\n",(0,s.jsx)(n.li,{children:"Sketch the kinematic chain"}),"\n",(0,s.jsx)(n.li,{children:"Describe the gripper type and actuation"}),"\n",(0,s.jsx)(n.li,{children:"Estimate the workspace size"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Document your analysis in a short paragraph (200 words)."}),"\n",(0,s.jsx)(n.h2,{id:"resources-for-deeper-learning",children:"Resources for Deeper Learning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Book"}),': "Introduction to Robotics" - John J. Craig (Chapters 2-3 on kinematics)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Video"}),': "Robot Arm Kinematics Explained" - MATLAB Tech Talks']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tool"}),": ROS Visualization Tools (",(0,s.jsx)(n.code,{children:"rviz"}),") for point clouds and sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Paper"}),': "A Survey of Robot Manipulation" - Siciliano & Khatib']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interactive"}),": OpenDynamics - simulate kinematics in browser"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": Week 3 - Control Systems Basics"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,s.jsx)(n.strong,{children:"Tip"}),": Try computing forward kinematics by hand for a 2-joint arm. Use simple trigonometry to find the end effector position for different joint angles!"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:function(e,n,r){r.d(n,{R:function(){return o},x:function(){return l}});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);