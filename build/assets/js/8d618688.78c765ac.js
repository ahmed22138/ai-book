"use strict";(self.webpackChunkai_textbook_frontend=self.webpackChunkai_textbook_frontend||[]).push([[186],{8453:function(e,n,i){i.d(n,{R:function(){return o},x:function(){return a}});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9697:function(e,n,i){i.r(n),i.d(n,{assets:function(){return l},contentTitle:function(){return a},default:function(){return h},frontMatter:function(){return o},metadata:function(){return t},toc:function(){return d}});var t=JSON.parse('{"id":"perception/week-4-computer-vision","title":"Week 4: Computer Vision Fundamentals","description":"Introduction to Robot Vision","source":"@site/docs/02-perception/week-4-computer-vision.mdx","sourceDirName":"02-perception","slug":"/perception/week-4-computer-vision","permalink":"/ai-textbook/docs/perception/week-4-computer-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/ai-textbook/tree/main/docs/02-perception/week-4-computer-vision.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Week 3: Control Systems Basics","permalink":"/ai-textbook/docs/introduction/week-3-control-systems"},"next":{"title":"Week 5: 3D Perception & Point Clouds","permalink":"/ai-textbook/docs/perception/week-5-3d-perception"}}'),r=i(4848),s=i(8453);const o={sidebar_position:1},a="Week 4: Computer Vision Fundamentals",l={},d=[{value:"Introduction to Robot Vision",id:"introduction-to-robot-vision",level:2},{value:"Why Vision Matters for Robots",id:"why-vision-matters-for-robots",level:3},{value:"Part 1: Image Fundamentals",id:"part-1-image-fundamentals",level:2},{value:"Representing Images as Data",id:"representing-images-as-data",level:3},{value:"Basic Image Operations",id:"basic-image-operations",level:3},{value:"Part 2: Feature Detection &amp; Matching",id:"part-2-feature-detection--matching",level:2},{value:"Image Gradients and Edges",id:"image-gradients-and-edges",level:3},{value:"Keypoint Detection",id:"keypoint-detection",level:3},{value:"Feature Matching",id:"feature-matching",level:3},{value:"Part 3: Object Detection",id:"part-3-object-detection",level:2},{value:"Bounding Box Detection",id:"bounding-box-detection",level:3},{value:"Part 4: Deep Learning for Vision (Neural Networks)",id:"part-4-deep-learning-for-vision-neural-networks",level:2},{value:"Convolutional Neural Networks (CNNs)",id:"convolutional-neural-networks-cnns",level:3},{value:"Pre-trained Models",id:"pre-trained-models",level:3},{value:"Real-World Example: Picking Ripe Tomatoes",id:"real-world-example-picking-ripe-tomatoes",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Week 4 Learning Outcomes",id:"week-4-learning-outcomes",level:2},{value:"Key Terminology",id:"key-terminology",level:2},{value:"Discussion Questions",id:"discussion-questions",level:2},{value:"Hands-On Activity",id:"hands-on-activity",level:2},{value:"Resources for Deeper Learning",id:"resources-for-deeper-learning",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"week-4-computer-vision-fundamentals",children:"Week 4: Computer Vision Fundamentals"})}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-robot-vision",children:"Introduction to Robot Vision"}),"\n",(0,r.jsxs)(n.p,{children:["Welcome to Module 2! This week begins our deep dive into ",(0,r.jsx)(n.strong,{children:"perception systems"}),"\u2014how robots understand the visual world around them. Computer vision is one of the most critical capabilities for embodied AI systems, enabling object recognition, spatial understanding, and scene comprehension."]}),"\n",(0,r.jsx)(n.h3,{id:"why-vision-matters-for-robots",children:"Why Vision Matters for Robots"}),"\n",(0,r.jsx)(n.p,{children:"Vision provides the richest information about the environment. While sensors like LIDAR measure distance well, cameras capture:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Color information"})," for object identification"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Texture and edges"})," for shape understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic meaning"})," through learned patterns"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"High-resolution detail"})," for precise manipulation"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"But vision is computationally expensive. This week, we explore the fundamental algorithms that make robot vision practical."}),"\n",(0,r.jsx)(n.h2,{id:"part-1-image-fundamentals",children:"Part 1: Image Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"representing-images-as-data",children:"Representing Images as Data"}),"\n",(0,r.jsxs)(n.p,{children:["A digital image is fundamentally a ",(0,r.jsx)(n.strong,{children:"matrix of pixel values"}),". For an RGB color image:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Image: 640\xd7480 pixels (width \xd7 height)\n\u251c\u2500 Red channel: 640\xd7480 matrix of values [0-255]\n\u251c\u2500 Green channel: 640\xd7480 matrix of values [0-255]\n\u2514\u2500 Blue channel: 640\xd7480 matrix of values [0-255]\n\nExample pixel (100, 200) = [R=255, G=128, B=0] \u2192 Orange color\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Image dimensions:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Grayscale: Height \xd7 Width \xd7 1"}),"\n",(0,r.jsx)(n.li,{children:"RGB Color: Height \xd7 Width \xd7 3"}),"\n",(0,r.jsx)(n.li,{children:"RGBA: Height \xd7 Width \xd7 4 (with transparency)"}),"\n",(0,r.jsx)(n.li,{children:"Typical resolution: 640\xd7480 (0.3MP), 1920\xd71080 (2MP), 4K (8MP)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"basic-image-operations",children:"Basic Image Operations"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom PIL import Image\n\nclass ImageProcessor:\n    def load_image(self, path):\n        """Load image from file"""\n        img = Image.open(path)\n        # Convert to numpy array: shape (height, width, 3)\n        return np.array(img)\n\n    def convert_to_grayscale(self, image):\n        """Convert RGB to grayscale"""\n        # Weighted average: 0.299\xd7R + 0.587\xd7G + 0.114\xd7B\n        gray = np.dot(image[...,:3], [0.299, 0.587, 0.114])\n        return gray.astype(np.uint8)\n\n    def resize_image(self, image, new_height, new_width):\n        """Resize image using bilinear interpolation"""\n        from scipy.ndimage import zoom\n        h, w = image.shape[:2]\n        scale_y = new_height / h\n        scale_x = new_width / w\n        return zoom(image, (scale_y, scale_x, 1), order=1)\n\n    def crop_image(self, image, y1, y2, x1, x2):\n        """Crop rectangular region from image"""\n        return image[y1:y2, x1:x2, :]\n\n    def apply_blur(self, image, kernel_size=5):\n        """Apply Gaussian blur to reduce noise"""\n        from scipy.ndimage import gaussian_filter\n        return np.stack([\n            gaussian_filter(image[:,:,i], sigma=kernel_size)\n            for i in range(image.shape[2])\n        ], axis=2).astype(np.uint8)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"part-2-feature-detection--matching",children:"Part 2: Feature Detection & Matching"}),"\n",(0,r.jsx)(n.h3,{id:"image-gradients-and-edges",children:"Image Gradients and Edges"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"gradient"})," of an image measures how quickly pixel intensity changes. Large gradients indicate edges."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Edge detection using Sobel operator:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class EdgeDetector:\n    def compute_sobel(self, image):\n        """Compute image gradients using Sobel operator"""\n\n        # Grayscale image\n        gray = self.convert_to_grayscale(image)\n\n        # Sobel kernels detect edges in X and Y directions\n        sobel_x = np.array([\n            [-1, 0, 1],\n            [-2, 0, 2],\n            [-1, 0, 1]\n        ]) / 8\n\n        sobel_y = np.array([\n            [-1, -2, -1],\n            [0, 0, 0],\n            [1, 2, 1]\n        ]) / 8\n\n        # Convolve image with kernels (simplified)\n        gx = self.convolve_2d(gray, sobel_x)\n        gy = self.convolve_2d(gray, sobel_y)\n\n        # Magnitude of gradient\n        magnitude = np.sqrt(gx**2 + gy**2)\n\n        # Direction of gradient (angle)\n        direction = np.arctan2(gy, gx) * 180 / np.pi\n\n        return magnitude, direction\n\n    def convolve_2d(self, image, kernel):\n        """Apply 2D convolution (simplified for clarity)"""\n        from scipy.signal import convolve2d\n        return convolve2d(image, kernel, mode=\'same\')\n\n    def detect_edges(self, image, threshold=100):\n        """Threshold gradient magnitude to find edges"""\n        magnitude, _ = self.compute_sobel(image)\n        edges = (magnitude > threshold).astype(np.uint8) * 255\n        return edges\n'})}),"\n",(0,r.jsx)(n.h3,{id:"keypoint-detection",children:"Keypoint Detection"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Keypoints"})," are distinctive locations in an image that can be reliably detected and matched across images."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"SIFT (Scale-Invariant Feature Transform) algorithm:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Build image pyramid (multiple scales)"}),"\n",(0,r.jsx)(n.li,{children:"Find candidate keypoints at local extrema"}),"\n",(0,r.jsx)(n.li,{children:"Refine with sub-pixel accuracy"}),"\n",(0,r.jsx)(n.li,{children:"Compute orientation-invariant descriptors"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class KeypointDetector:\n    def detect_sift_keypoints(self, image):\n        """Detect SIFT keypoints in image"""\n\n        # Build image pyramid (scale space)\n        pyramid = self.build_gaussian_pyramid(image, num_levels=5)\n\n        keypoints = []\n\n        # Find local extrema (minima/maxima)\n        for level in range(1, len(pyramid)-1):\n            prev_level = pyramid[level-1]\n            curr_level = pyramid[level]\n            next_level = pyramid[level+1]\n\n            # For each pixel in current level\n            for y in range(1, curr_level.shape[0]-1):\n                for x in range(1, curr_level.shape[1]-1):\n\n                    # Get 3\xd73\xd73 neighborhood\n                    value = curr_level[y, x]\n\n                    # Check if local extremum\n                    neighborhood_prev = prev_level[y-1:y+2, x-1:x+2]\n                    neighborhood_curr = curr_level[y-1:y+2, x-1:x+2]\n                    neighborhood_next = next_level[y-1:y+2, x-1:x+2]\n\n                    is_extremum = (\n                        np.all(value > np.concatenate([\n                            neighborhood_prev.flatten(),\n                            neighborhood_curr.flatten(),\n                            neighborhood_next.flatten()\n                        ])) or\n                        np.all(value < np.concatenate([\n                            neighborhood_prev.flatten(),\n                            neighborhood_curr.flatten(),\n                            neighborhood_next.flatten()\n                        ]))\n                    )\n\n                    if is_extremum:\n                        keypoints.append({\n                            \'x\': x,\n                            \'y\': y,\n                            \'scale\': level,\n                            \'magnitude\': abs(value)\n                        })\n\n        return keypoints\n\n    def build_gaussian_pyramid(self, image, num_levels=5):\n        """Build Gaussian pyramid for scale-space analysis"""\n        pyramid = [image]\n        for _ in range(num_levels-1):\n            # Blur and downsample\n            blurred = gaussian_filter(pyramid[-1], sigma=1.0)\n            downsampled = blurred[::2, ::2]  # Every other pixel\n            pyramid.append(downsampled)\n        return pyramid\n'})}),"\n",(0,r.jsx)(n.h3,{id:"feature-matching",children:"Feature Matching"}),"\n",(0,r.jsx)(n.p,{children:"Once keypoints are detected, we match them across images to track objects or estimate motion."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class FeatureMatcher:\n    def match_keypoints(self, keypoints1, descriptors1,\n                       keypoints2, descriptors2):\n        \"\"\"Match keypoints between two images\"\"\"\n\n        matches = []\n\n        for i, desc1 in enumerate(descriptors1):\n            # Find closest descriptor in image 2\n            distances = np.linalg.norm(\n                descriptors2 - desc1, axis=1\n            )\n            closest_idx = np.argmin(distances)\n            distance_closest = distances[closest_idx]\n\n            # Find second closest (for Lowe's ratio test)\n            sorted_indices = np.argsort(distances)\n            distance_second = distances[sorted_indices[1]]\n\n            # Lowe's ratio test: reject if closest is not clearly better\n            if distance_closest / distance_second < 0.7:\n                matches.append({\n                    'idx1': i,\n                    'idx2': closest_idx,\n                    'distance': distance_closest,\n                    'keypoint1': keypoints1[i],\n                    'keypoint2': keypoints2[closest_idx]\n                })\n\n        return matches\n"})}),"\n",(0,r.jsx)(n.h2,{id:"part-3-object-detection",children:"Part 3: Object Detection"}),"\n",(0,r.jsx)(n.h3,{id:"bounding-box-detection",children:"Bounding Box Detection"}),"\n",(0,r.jsx)(n.p,{children:"The simplest object detection: find rectangular regions containing objects."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class ObjectDetector:\n    def detect_by_color(self, image, target_color, tolerance=30):\n        \"\"\"Detect objects by color\"\"\"\n\n        # Define color range in HSV\n        # (HSV is more robust than RGB for color)\n        target_hsv = self.rgb_to_hsv(target_color)\n\n        hsv_image = self.rgb_to_hsv_image(image)\n\n        # Find pixels within color range\n        mask = self.create_color_mask(\n            hsv_image, target_hsv, tolerance\n        )\n\n        # Find connected components (clusters of pixels)\n        labeled, num_objects = self.label_connected_components(mask)\n\n        bounding_boxes = []\n        for obj_id in range(1, num_objects+1):\n            # Find bounding box for this object\n            coords = np.where(labeled == obj_id)\n            if len(coords[0]) > 50:  # Minimum size\n                y_min, y_max = coords[0].min(), coords[0].max()\n                x_min, x_max = coords[1].min(), coords[1].max()\n\n                bounding_boxes.append({\n                    'x_min': int(x_min),\n                    'y_min': int(y_min),\n                    'x_max': int(x_max),\n                    'y_max': int(y_max),\n                    'width': int(x_max - x_min),\n                    'height': int(y_max - y_min),\n                    'center_x': int((x_min + x_max) / 2),\n                    'center_y': int((y_min + y_max) / 2),\n                    'area': len(coords[0])\n                })\n\n        return bounding_boxes, mask\n\n    def draw_bounding_boxes(self, image, boxes, color=(0, 255, 0)):\n        \"\"\"Draw detected bounding boxes on image\"\"\"\n        import cv2\n        img_copy = image.copy()\n        for box in boxes:\n            cv2.rectangle(\n                img_copy,\n                (box['x_min'], box['y_min']),\n                (box['x_max'], box['y_max']),\n                color, 2\n            )\n            cv2.putText(\n                img_copy,\n                f\"A={box['area']}\",\n                (box['x_min'], box['y_min']-10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1\n            )\n        return img_copy\n"})}),"\n",(0,r.jsx)(n.h2,{id:"part-4-deep-learning-for-vision-neural-networks",children:"Part 4: Deep Learning for Vision (Neural Networks)"}),"\n",(0,r.jsxs)(n.p,{children:["While traditional computer vision (edges, features) works well for specific tasks, ",(0,r.jsx)(n.strong,{children:"deep learning"})," enables learning what features matter for your task."]}),"\n",(0,r.jsx)(n.h3,{id:"convolutional-neural-networks-cnns",children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,r.jsxs)(n.p,{children:["A CNN learns ",(0,r.jsx)(n.strong,{children:"filters"})," (like our Sobel edge detector) automatically from data."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class SimpleCNN:\n    def __init__(self):\n        """Initialize simple CNN architecture"""\n        # This is pseudo-code; real implementation uses PyTorch/TensorFlow\n\n        # Layer 1: Convolution (32 filters, 3\xd73 kernel)\n        self.conv1 = ConvLayer(input_channels=3, output_channels=32,\n                              kernel_size=3, activation=\'relu\')\n\n        # Layer 2: Max pooling (2\xd72)\n        self.pool1 = MaxPoolLayer(kernel_size=2)\n\n        # Layer 3: Convolution (64 filters, 3\xd73 kernel)\n        self.conv2 = ConvLayer(input_channels=32, output_channels=64,\n                              kernel_size=3, activation=\'relu\')\n\n        # Layer 4: Max pooling (2\xd72)\n        self.pool2 = MaxPoolLayer(kernel_size=2)\n\n        # Layer 5: Fully connected (128 neurons)\n        self.fc1 = FullyConnectedLayer(input_size=64*56*56,\n                                       output_size=128,\n                                       activation=\'relu\')\n\n        # Layer 6: Output (10 classes)\n        self.fc2 = FullyConnectedLayer(input_size=128,\n                                       output_size=10,\n                                       activation=\'softmax\')\n\n    def forward(self, image):\n        """Forward pass through network"""\n        # Input: 224\xd7224\xd73 image\n        x = self.conv1(image)      # \u2192 224\xd7224\xd732\n        x = self.pool1(x)          # \u2192 112\xd7112\xd732\n        x = self.conv2(x)          # \u2192 112\xd7112\xd764\n        x = self.pool2(x)          # \u2192 56\xd756\xd764\n        x = x.flatten()            # \u2192 200704\n        x = self.fc1(x)            # \u2192 128\n        x = self.fc2(x)            # \u2192 10\n        return x\n'})}),"\n",(0,r.jsx)(n.h3,{id:"pre-trained-models",children:"Pre-trained Models"}),"\n",(0,r.jsxs)(n.p,{children:["Most practitioners use ",(0,r.jsx)(n.strong,{children:"pre-trained models"})," trained on massive datasets:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ResNet-50"}),": Trained on ImageNet (1M images, 1000 classes)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLO"}),": Trained for real-time object detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MobileNet"}),": Optimized for mobile/embedded devices"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class PretrainedDetector:\n    def __init__(self, model_name='yolo-v8'):\n        \"\"\"Load pre-trained model\"\"\"\n        from ultralytics import YOLO\n\n        # Download and load model\n        self.model = YOLO(f'{model_name}.pt')\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects using pre-trained model\"\"\"\n\n        # Run inference\n        results = self.model.predict(image, conf=0.5)\n\n        detections = []\n        for result in results:\n            for box in result.boxes:\n                detection = {\n                    'class_name': self.model.names[int(box.cls)],\n                    'confidence': float(box.conf),\n                    'x_min': int(box.xyxy[0, 0]),\n                    'y_min': int(box.xyxy[0, 1]),\n                    'x_max': int(box.xyxy[0, 2]),\n                    'y_max': int(box.xyxy[0, 3]),\n                }\n                detections.append(detection)\n\n        return detections\n"})}),"\n",(0,r.jsx)(n.h2,{id:"real-world-example-picking-ripe-tomatoes",children:"Real-World Example: Picking Ripe Tomatoes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class TomatoRobotVision:\n    def __init__(self):\n        self.color_detector = ObjectDetector()\n        self.ripeness_classifier = PretrainedDetector('tomato-ripeness')\n        self.camera = CameraInterface()\n\n    def find_ripe_tomatoes(self):\n        \"\"\"Detect ripe tomatoes in greenhouse\"\"\"\n\n        # Capture image\n        image = self.camera.capture()\n\n        # Step 1: Detect red objects (tomato color range)\n        red_color = np.array([0, 100, 255])  # HSV red\n        tomato_regions, mask = self.color_detector.detect_by_color(\n            image, red_color, tolerance=30\n        )\n\n        # Step 2: Classify ripeness for each candidate\n        ripe_tomatoes = []\n\n        for region in tomato_regions:\n            # Crop region around potential tomato\n            y1 = max(0, region['y_min']-10)\n            y2 = min(image.shape[0], region['y_max']+10)\n            x1 = max(0, region['x_min']-10)\n            x2 = min(image.shape[1], region['x_max']+10)\n\n            crop = image[y1:y2, x1:x2, :]\n\n            # Classify ripeness\n            ripeness = self.ripeness_classifier.detect_objects(crop)\n\n            if ripeness and ripeness[0]['class_name'] == 'ripe':\n                ripe_tomatoes.append({\n                    **region,\n                    'ripeness_score': ripeness[0]['confidence']\n                })\n\n        return ripe_tomatoes\n\n    def visualize_results(self, image, tomatoes):\n        \"\"\"Draw results on image\"\"\"\n        img_vis = image.copy()\n\n        for tom in tomatoes:\n            # Draw green box for ripe tomato\n            cv2.rectangle(\n                img_vis,\n                (tom['x_min'], tom['y_min']),\n                (tom['x_max'], tom['y_max']),\n                (0, 255, 0), 2\n            )\n            cv2.putText(\n                img_vis,\n                f\"Ripe ({tom['ripeness_score']:.2f})\",\n                (tom['x_min'], tom['y_min']-5),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1\n            )\n\n        return img_vis\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsxs)(n.p,{children:["Real-time vision on robots requires ",(0,r.jsx)(n.strong,{children:"balancing accuracy and speed"}),"."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Approach"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"Suitable For"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Color segmentation"}),(0,r.jsx)(n.td,{children:"100+ FPS"}),(0,r.jsx)(n.td,{children:"Low"}),(0,r.jsx)(n.td,{children:"Simple environments"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Edge detection"}),(0,r.jsx)(n.td,{children:"50+ FPS"}),(0,r.jsx)(n.td,{children:"Low-Medium"}),(0,r.jsx)(n.td,{children:"Feature-rich scenes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Traditional features (SIFT)"}),(0,r.jsx)(n.td,{children:"10-30 FPS"}),(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"Tracking, matching"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Small CNN (MobileNet)"}),(0,r.jsx)(n.td,{children:"30-60 FPS"}),(0,r.jsx)(n.td,{children:"Medium-High"}),(0,r.jsx)(n.td,{children:"Edge devices"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Large CNN (ResNet-50)"}),(0,r.jsx)(n.td,{children:"5-15 FPS"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"GPU-enabled systems"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLO (real-time detector)"}),(0,r.jsx)(n.td,{children:"30-60 FPS"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Object detection"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"week-4-learning-outcomes",children:"Week 4 Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this week, you should be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explain"})," how digital images are represented as matrices of pixel values"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement"})," basic image processing (resize, crop, blur, threshold)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Detect"})," edges using gradient-based methods (Sobel operator)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Identify"})," keypoints and match them across images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply"})," color-based object detection for structured environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compare"})," traditional and deep learning approaches to vision"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Term"}),(0,r.jsx)(n.th,{children:"Definition"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Pixel"})}),(0,r.jsx)(n.td,{children:"Single point in image; RGB image has 3 values per pixel"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Kernel"})}),(0,r.jsx)(n.td,{children:"Small matrix (filter) convolved with image for feature extraction"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Convolution"})}),(0,r.jsx)(n.td,{children:"Mathematical operation to apply filter across image"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Edge"})}),(0,r.jsx)(n.td,{children:"Location where pixel intensity changes rapidly"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Keypoint"})}),(0,r.jsx)(n.td,{children:"Distinctive, repeatable location in image (SIFT, SURF)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Descriptor"})}),(0,r.jsx)(n.td,{children:"Feature vector describing appearance around keypoint"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Bounding box"})}),(0,r.jsx)(n.td,{children:"Rectangular region marking object location"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"CNN"})}),(0,r.jsx)(n.td,{children:"Convolutional Neural Network with learned filters"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"discussion-questions",children:"Discussion Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tradeoffs"}),": Why is color-based detection faster but less robust than CNN-based detection? When would you use each?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real-time constraints"}),": A robot must detect objects at 30 FPS. If processing takes 100ms per frame, what percentage of time budget is spent on vision?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": A tomato detector works perfectly indoors under controlled lighting. What happens outdoors? How would you make it more robust?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained models"}),": When should you use pre-trained models vs. train your own? What are the tradeoffs?"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-activity",children:"Hands-On Activity"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Build a simple color detector"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Capture or download an image with distinct colored objects"}),"\n",(0,r.jsx)(n.li,{children:"Implement color-based object detection (no deep learning)"}),"\n",(0,r.jsx)(n.li,{children:"Draw bounding boxes around detected objects"}),"\n",(0,r.jsxs)(n.li,{children:["Test robustness by:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Changing lighting"}),"\n",(0,r.jsx)(n.li,{children:"Rotating image"}),"\n",(0,r.jsx)(n.li,{children:"Adding objects with similar colors"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Document detection accuracy (true positives / total objects)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"resources-for-deeper-learning",children:"Resources for Deeper Learning"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Book"}),': "Computer Vision: Algorithms and Applications" - Richard Szeliski']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Course"}),': "Computer Vision Basics" - OpenCV tutorials']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tool"}),": OpenCV (C++/Python) for traditional vision"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Framework"}),": PyTorch or TensorFlow for deep learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dataset"}),": COCO dataset for training object detectors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained models"}),": Hugging Face Model Hub"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": Week 5 - Advanced Vision & 3D Perception"]}),"\n",(0,r.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,r.jsx)(n.strong,{children:"Tip"}),": Start with color-based detection before diving into neural networks. Understanding traditional vision builds intuition for why deep learning works!"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);